{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Who am i?","text":"<pre><code>Name: Ton Machielsen\ne-Mail: ton@machielsen.net\nWWW: https://toontjem.github.io (this site)\nGithub: https://www.github.com/ToontjeM\nLinkedIn: https://wwww.linkedin.com/in/tonmachielsen\n\nNo other social media.\n</code></pre>"},{"location":"#things-i-have-worked-on","title":"Things i have worked on","text":"Loading repositories..."},{"location":"Install_controlplane/","title":"Installation EDB Postgres AI Control Plane on kind","text":"","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"Install_controlplane/#pre-reqs","title":"Pre-reqs","text":"<ul> <li>git (OS repos)</li> <li>make (OS repos)</li> <li>kind (https://kind.sigs.k8s.io/)</li> <li>helm (https://helm.sh/)</li> </ul>","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"Install_controlplane/#installation","title":"Installation","text":"<p>Tip</p> <p>TL;DR Just run the <code>00-provision.sh</code> script which does all the below and go take a \u2615.</p> <p>Script looks like this: <pre><code>#!/bin/bash\n\nmake create-kind\nEDBTOKEN=(`cat $HOME/.edbtoken`)\necho $EDBTOKEN | ./scripts/install-static-pull-secret.sh\nhelm upgrade -n edbpgai-bootstrap \\\n--install -f deploy/charts/edbpgai-bootstrap/values-kind.yaml \\\n--set bootstrapImage=docker.enterprisedb.com/staging_pgai-platform/edbpgai-bootstrap/bootstrap-kind:v1.0.0-gm-appl \\\n--set remoteContainerRegistryURL=docker.enterprisedb.com/staging_pgai-platform \\\n--set internalContainerRegistryURL=docker.enterprisedb.com/staging_pgai-platform \\\nbootstrap deploy/charts/edbpgai-bootstrap\nPOD=$(kubectl get pods -n edbpgai-bootstrap -o custom-columns=\":metadata.name\" | grep -v \"NAME\")\nkubectl logs -n edbpgai-bootstrap $POD -f\n</code></pre></p> <p><code>git clone https://github.com/EnterpriseDB/edbpgai-boot</code></p> <p><code>cd edbpgai-boot</code></p> <p><code>export EDBToken=(`cat $HOME/.edbtoken`)</code></p> <p><code>make create-kind</code></p> <pre><code>bash /Users/ton.machielsen/edbpgai-bootstrap/scripts/create-kind.sh /Users/ton.machielsen/edbpgai-bootstrap/bin/kind-v0.24.0 /Users/ton.machielsen/edbpgai-bootstrap/kind-config.yaml\nCreating cluster \"edbpgai\" ...\n \u2713 Ensuring node image (kindest/node:v1.29.8) \ud83d\uddbc\n \u2713 Preparing nodes \ud83d\udce6 \ud83d\udce6 \ud83d\udce6 \ud83d\udce6  \n \u2713 Writing configuration \ud83d\udcdc \n \u2713 Starting control-plane \ud83d\udd79\ufe0f \n \u2713 Installing CNI \ud83d\udd0c \n \u2713 Installing StorageClass \ud83d\udcbe \n \u2713 Joining worker nodes \ud83d\ude9c \nSet kubectl context to \"kind-edbpgai\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-edbpgai\n\nHave a nice day! \ud83d\udc4b\n</code></pre> <p><code>echo $EDBTOKEN | ./scripts/install-static-pull-secret.sh</code></p> <p><pre><code>Enter the password for staging_beaconator@docker.enterprisedb.com\nCreating secret edb-cred\nnamespace/upm-replicator created\nsecret/edb-cred created\nnamespace/edbpgai-bootstrap created\nsecret/edb-cred created\nsecret/edb-cred annotated\nInstallation completed\n</code></pre> Run the bootstrap using helm. <pre><code>helm upgrade -n edbpgai-bootstrap \\\n  --install -f deploy/charts/edbpgai-bootstrap/values-kind.yaml \\\n  --set bootstrapImage=docker.enterprisedb.com/staging_pgai-platform/edbpgai-bootstrap/bootstrap-kind:v1.0.0-gm-appl \\\n  --set remoteContainerRegistryURL=docker.enterprisedb.com/staging_pgai-platform \\\n  --set internalContainerRegistryURL=docker.enterprisedb.com/staging_pgai-platform \\\n  bootstrap deploy/charts/edbpgai-bootstrap\n</code></pre></p> <p>Note</p> <p>This will take appx 15 minutes to complete. You can follow the progess using <code>kubectl logs -n edbpgai-bootstrap edbpgai-bootstrap-job-v1.0.0-&lt;tab&gt; -f</code></p> <pre><code>...\n\n12:32:20PM: create customresourcedefinition/filters.fluentbit.fluent.io (apiextensions.k8s.io/v1) cluster\n12:32:20PM: create customresourcedefinition/clusterfluentdconfigs.fluentd.fluent.io (apiextensions.k8s.io/v1) cluster\n12:32:20PM: create customresourcedefinition/clusterinputs.fluentbit.fluent.io (apiextensions.k8s.io/v1) cluster\n12:32:22PM: create customresourcedefinition/clusterinputs.fluentd.fluent.io (apiextensions.k8s.io/v1) cluster\n12:32:25PM: create secret/loki-http-pass (v1) namespace: logging\n12:32:25PM: create customresourcedefinition/outputs.fluentd.fluent.io (apiextensions.k8s.io/v1) cluster\n12:32:27PM: create customresourcedefinition/fluentds.fluentd.fluent.io (apiextensions.k8s.io/v1) cluster\n12:32:27PM: ---- waiting on 30 changes [0/39 done] ----\n-main- +2 -2 (base) \u221a ~/edbpgai-bootstrap\n</code></pre> <p>The UI is now available at http://localhost:8000</p> <p>Currently the UI can only be accessed from the host where it is installed or using X-forwarding (which is terribly slow).</p> <p>Login using <code>owner@mycompany.com</code> with password <code>password</code> and create a cluster.</p> <p>Once a cluster is created you will see the following: </p> <p>Click on the cluster and in overview you will see something like this: </p> <p>Since the pgai.com domain doesn't exist and the port 30495 isn't accessible, do the following: - Check out what is the name of the cluster using <code>kubectl get clusters -A</code>. The cluster you are looking for tis the one that starts with the <code>p-</code>.  ``` NAMESPACE                NAME                       AGE   INSTANCES   READY   STATUS                     PRIMARY edb-migration-portal     cluster-migration-portal   53m   1           1       Cluster in healthy state   cluster-migration-portal-1 *** p-bvhzn7zaie ***     p-bvhzn7zaie               30m   1           1       Cluster in healthy state   p-bvhzn7zaie-1 transporter-ui-service   transporter-db             40m   1           1       Cluster in healthy state   transporter-db-1 upm-beaco-ff-base        app-db                     46m   1           1       Cluster in healthy state   app-db-1 upm-beacon               beacondb                   46m   1           1       Cluster in healthy state   beacondb-1 <pre><code>- Forward port 5432 in that cluster to 5433 on your local machine using `kubectl port-forward -n p-bvhzn7zaie services/p-bvhzn7zaie-rw-external 5433:5432 --address 0.0.0.0`\n\nNow that port 5433 is forwarded to your PC you can run `psql -h localhost -p 5433 -U edb_admin postgres`.\n</code></pre> Password for user edb_admin:  psql (17.0 (Homebrew petere/postgresql), server 17.2 (Debian 17.2-1EDB.bookworm)) SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off, ALPN: postgresql) Type \"help\" for help.</p> <p>postgres=# ```</p>","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"Kafka_on_AWS/","title":"Kafka on AWS","text":"<pre><code>I want to deploy an environment in AWS.\nThis environment will contain three EC2 instances called publisher, broker and subscriber.\nOn publisher i want to run two python scripts which sends messages to the Kafka broker on instance broker.\nOne script sends a continuous stream of letters to the yopic letters on the broker, one script sends a contiinous stream of numbers to the toopic numbers on the broker.\nOn broker i want to run the Kafka software configured to run two topics called letters and numbers.\nOn subscriber i want to run three python scripts:\n- Script 1 subscribes to the numbers topic and displays what it receives.\n- Script 2 subscribes to the numbers topic and displays what it receives.\n- Script 3 subscribes to both topics and displays what it receives.\nAll scripts can be console based.\n</code></pre> <p>Certainly! Below is a detailed guide on how to deploy the environment you've described in AWS, utilizing Amazon EC2 instances for your Publisher, Broker, and Subscriber. Below are the steps you'll need to follow.</p>","tags":["AWS","Kafka","Hobby"]},{"location":"Kafka_on_AWS/#step-1-set-up-your-aws-account","title":"Step 1: Set Up Your AWS Account","text":"<ol> <li>Sign in to your AWS account or create one if you do not have an account.</li> <li>Ensure you have appropriate permissions to launch EC2 instances and create security groups.</li> </ol>","tags":["AWS","Kafka","Hobby"]},{"location":"Kafka_on_AWS/#step-2-create-ec2-instances","title":"Step 2: Create EC2 Instances","text":"<ol> <li>Launch EC2 Instances:<ul> <li>Go to the EC2 Dashboard.</li> <li>Click on Launch Instance.</li> <li>Choose an Amazon Machine Image (AMI), ideally a lightweight Linux distribution like Amazon Linux 2 or Ubuntu.</li> <li>Choose an instance type (e.g., t2.micro for testing).</li> <li>Configure instance details, including the desired number of instances (3 instances).</li> <li>Set up your storage and tags if desired.</li> </ul> </li> <li>Configure Security Group:<ul> <li>Create a new security group allowing the following inbound rules:</li> <li>SSH (port 22): from your IP (for management).</li> <li>Kafka (default port 9092): from any IP or limited to the publishers/subscribers\u2019 IPs.</li> <li>Customize other necessary ports based on your needs.</li> </ul> </li> <li>Assign Public IP Addresses:</li> <li>Ensure that each instance has a public IP assigned.</li> </ol>","tags":["AWS","Kafka","Hobby"]},{"location":"Kafka_on_AWS/#step-3-install-kafka-on-the-broker-instance","title":"Step 3: Install Kafka on the Broker Instance","text":"<ol> <li> <p>SSH into the Broker instance:</p> <p><code>ssh -i your-key.pem ec2-user@your-broker-public-ip</code></p> </li> <li> <p>Install Java (Kafka requires Java):</p> <p><code>sudo yum update -y</code>  # For Amazon Linux <code>sudo yum install java -y</code></p> </li> <li> <p>Download and Install Kafka:</p> <p><code>wget http://apache.mirrors.pair.com/kafka/2.13-2.8.0/kafka_2.13-2.8.0.tgz</code></p> <p><code>tar -xvf kafka_2.13-2.8.0.tgz</code></p> <p><code>cd kafka_2.13-2.8.0</code></p> </li> <li> <p>Start Kafka Server:</p> </li> <li> <p>Start the Zookeeper server:</p> <p><code>bin/zookeeper-server-start.sh config/zookeeper.properties &amp;</code></p> </li> <li> <p>Start the Kafka server:</p> <p><code>bin/kafka-server-start.sh config/server.properties &amp;</code></p> </li> <li> <p>Create Topics:</p> </li> <li> <p>Run the following commands to create topics:</p> <p><code>bin/kafka-topics.sh --create --topic letters --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1</code></p> <p><code>bin/kafka-topics.sh --create --topic numbers --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1</code></p> </li> </ol>","tags":["AWS","Kafka","Hobby"]},{"location":"Kafka_on_AWS/#step-4-set-up-the-publisher-instance","title":"Step 4: Set Up the Publisher Instance","text":"<ol> <li> <p>SSH into the Publisher instance:</p> <p><code>ssh -i your-key.pem ec2-user@your-publisher-public-ip</code></p> </li> <li> <p>Install Python &amp; Necessary Packages:</p> <p><code>sudo yum install python3 -y</code></p> <p><code>pip3 install kafka-python</code></p> </li> <li> <p>Create Python Scripts:</p> </li> <li> <p>Create publish_letters.py: <pre><code>from kafka import KafkaProducer\nimport time\nimport string\n\nproducer = KafkaProducer(bootstrap_servers='your-broker-public-ip:9092')\n\nwhile True:\n    for letter in string.ascii_letters:\n        producer.send('letters', value=letter.encode())\n        time.sleep(1)\n</code></pre></p> </li> <li> <p>Create publish_numbers.py:</p> </li> </ol> <pre><code>from kafka import KafkaProducer\nimport time\n\nproducer = KafkaProducer(bootstrap_servers='your-broker-public-ip:9092')\n\nnum = 1\nwhile True:\n    producer.send('numbers', value=str(num).encode())\n    num += 1\n    time.sleep(1)\n</code></pre>","tags":["AWS","Kafka","Hobby"]},{"location":"Kafka_on_AWS/#step-5-set-up-the-subscriber-instance","title":"Step 5: Set Up the Subscriber Instance","text":"<ol> <li> <p>SSH into the Subscriber instance:</p> <p><code>ssh -i your-key.pem ec2-user@your-subscriber-public-ip</code></p> </li> <li> <p>Install Python &amp; Necessary Packages:</p> <p><code>sudo yum install python3 -y</code></p> <p><code>pip3 install kafka-python</code></p> </li> </ol> <p>3, Create Python Scripts:</p> <ul> <li> <p>Create subscribe_numbers.py: <pre><code>from kafka import KafkaConsumer\n\nconsumer = KafkaConsumer('numbers', bootstrap_servers='your-broker-public-ip:9092')\n\nfor message in consumer:\n    print(f\"Received number: {message.value.decode()}\")\n</code></pre></p> </li> <li> <p>Create subscribe_letters.py: <pre><code>from kafka import KafkaConsumer\n\nconsumer = KafkaConsumer('letters', bootstrap_servers='your-broker-public-ip:9092')\n\nfor message in consumer:\n    print(f\"Received letter: {message.value.decode()}\")\n</code></pre></p> </li> <li> <p>Create subscribe_both.py: <pre><code>from kafka import KafkaConsumer\n\nconsumer = KafkaConsumer('letters', 'numbers', bootstrap_servers='your-broker-public-ip:9092')\n\nfor message in consumer:\n    print(f\"Received {message.topic}: {message.value.decode()}\")\n</code></pre></p> </li> </ul>","tags":["AWS","Kafka","Hobby"]},{"location":"Kafka_on_AWS/#step-6-run-your-scripts","title":"Step 6: Run Your Scripts","text":"<ol> <li> <p>On the Publisher Instance:</p> </li> <li> <p>Open two terminal sessions and run:</p> </li> </ol> <p><code>python3 publish_letters.py</code></p> <pre><code>`python3 publish_numbers.py`\n</code></pre> <ol> <li> <p>On the Subscriber Instance:</p> </li> <li> <p>Open three terminal sessions and run:</p> <p><code>python3 subscribe_numbers.py</code></p> <p><code>python3 subscribe_letters.py</code></p> <p><code>python3 subscribe_both.py</code></p> </li> </ol>","tags":["AWS","Kafka","Hobby"]},{"location":"Kafka_on_AWS/#step-7-verify-communication","title":"Step 7: Verify Communication","text":"<p>Watch the subscriber instances for messages being printed from the corresponding topics. Monitor the publisher instance to ensure letters and numbers are being sent to the correct topics.</p>","tags":["AWS","Kafka","Hobby"]},{"location":"Kafka_on_AWS/#conclusion","title":"Conclusion","text":"<p>You now have a fully functional Kafka setup on AWS with three EC2 instances for the publisher, broker, and subscriber. Each component is running its intended scripts and you can expand this further based on your requirements. Make sure to manage AWS resources properly to avoid unnecessary costs and shut down instances when not needed.</p>","tags":["AWS","Kafka","Hobby"]},{"location":"Ollama_with_HuggingFace_models/","title":"How to Use Hugging Face Models with Ollama","text":"","tags":["AI","Hobby","Work"]},{"location":"Ollama_with_HuggingFace_models/#example","title":"Example","text":"<p>Model is LLama-3.1-8B-Lexi-Uncensored-V2-GGUF. - Go to (https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2-GGUF) - Download one of the GGUF model files to your computer. The bigger the higher quality and uses more resources. - Click on <code>Files and Versions</code> on the model page - Save the file - Open a terminal where you put that file and create a Modelfile using <code>vi Modelfile</code> - Add a FROM and SYSTEM section to the file. The FROM points to your model file you just downloaded, and the SYSTEM prompt is the core model instructions for it to follow on every request. - Create a new LLM using <code>ollama create lexiwriter</code> - Run your new model.</p>","tags":["AI","Hobby","Work"]},{"location":"PG_vs_Mongo/","title":"PostgreSQL vs. MongoDB","text":"","tags":["MongoDB","PostgreSQL","Work"]},{"location":"PG_vs_Mongo/#postgres","title":"Postgres","text":"","tags":["MongoDB","PostgreSQL","Work"]},{"location":"PG_vs_Mongo/#set-up-database","title":"Set up database","text":"<p><pre><code>postgres=# create database test;\nCREATE DATABASE\npostgres=# \\c test\nYou are now connected to database \"test\" as user \"postgres\".\n</code></pre> <pre><code>test=# DROP TABLE IF EXISTS estados CASCADE;\nDROP TABLE IF EXISTS transaction;\n\n-- Create the first table, transaction\nCREATE TABLE transaction (\n    id SERIAL PRIMARY KEY,  \n    fecha DATE NOT NULL,\n    bicOrdenante BIGINT NOT NULL,\n    bicBeneficiario BIGINT NOT NULL,\n    referencia VARCHAR(255),\n    endToEndId VARCHAR(35),\n    instrId VARCHAR(35),\n    ibanOrdenante VARCHAR(34) NOT NULL,\n    ibanBeneficiario VARCHAR(34) NOT NULL\n);\n\n-- Create the second table, estados with a foreign key reference to transaction\nCREATE TABLE estados (\n    id SERIAL PRIMARY KEY, \n    transaction_id INT NOT NULL,  \n    estado VARCHAR(255) NOT NULL,\n    timestamp TIMESTAMP NOT NULL,\n    xml TEXT NOT NULL,\n    FOREIGN KEY (transaction_id) REFERENCES transaction(id) ON DELETE CASCADE  \n);\n</code></pre> <pre><code>my_database=# DO $$\nDECLARE\n    i INT;\nBEGIN\n    FOR i IN 1..10000 LOOP\n        INSERT INTO transaction (fecha, bicOrdenante, bicBeneficiario, referencia, endToEndId, instrId, ibanOrdenante, ibanBeneficiario)\n        VALUES (\n            CURRENT_DATE - (random() * 365)::int,  -- Random date within the last year\n            (8000000000 + (random() * 1000000000)::BIGINT),   -- Random BIC as BIGINT\n            (8000000000 + (random() * 1000000000)::BIGINT),   -- Random BIC as BIGINT\n            substr(md5(random()::text), 1, 255),   -- Random reference string (255 chars)\n            substr(md5(random()::text), 1, 35),    -- Random EndToEndId (35 chars)\n            substr(md5(random()::text), 1, 35),    -- Random InstrId (35 chars)\n            'GB29NWBK60161331926819' || i::text,   -- Fixed length IBAN\n            'GB29NWBK60161331926819' || (i + 100)::text -- Fixed length IBAN with offset\n        );\n    END LOOP;\nEND $$;\n\nDO $$\nDECLARE\n    trans_id INT;\n    estado_count INT;\n    j INT;\nBEGIN\n    FOR trans_id IN 1..10000 LOOP\n        -- Generate a random number of estados (between 1 and 18)\n        estado_count := (random() * 18 + 1)::int;\n\n        FOR j IN 1..estado_count LOOP\n            INSERT INTO estados (transaction_id, estado, timestamp, xml)\n            VALUES (\n                trans_id,\n                substr(md5(random()::text), 1, 255),\n                CURRENT_TIMESTAMP - (random() * 1000000)::int * INTERVAL '1 second',\n                '&lt;?xml version=\"1.0\"?&gt;&lt;data&gt;Random XML data for estado ' || j || '&lt;/data&gt;'\n            );\n        END LOOP;\n    END LOOP;\nEND $$;\n</code></pre></p>","tags":["MongoDB","PostgreSQL","Work"]},{"location":"PG_vs_Mongo/#query-database","title":"Query database","text":"<pre><code>psql -d my_database -c \"\\timing; SELECT * FROM transaction t LEFT JOIN estados e ON t.id = e.transaction_id;\" &gt; /dev/null\n</code></pre>","tags":["MongoDB","PostgreSQL","Work"]},{"location":"PG_vs_Mongo/#mongodb","title":"MongoDB","text":"","tags":["MongoDB","PostgreSQL","Work"]},{"location":"PG_vs_Mongo/#set-up-database_1","title":"Set up database","text":"<pre><code>use my_database;\ndb.transactions.drop();\nfor (let i = 1; i &lt;= 10000; i++) {\n    // Generate random number of estados (between 1 and 18)\n    const estadoCount = Math.floor(Math.random() * 18) + 1;\n\n    // Initialize the estados array\n    const estados = [];\n\n    for (let j = 0; j &lt; estadoCount; j++) {\n        estados.push({\n            estado: Math.random().toString(36).substring(2, 15), // Random estado string\n            timestamp: new Date(Date.now() - (Math.random() * 10000000)), // Random timestamp\n            xml: `&lt;data&gt;Random XML data for estado ${j + 1}&lt;/data&gt;` // Sample XML\n        });\n    }\n\n    // Create a transaction document\n    const transaction = {\n        fecha: new Date(Date.now() - (Math.random() * 31536000000)), // Random date within the last year\n        bicOrdenante: Math.floor(8000000000 + Math.random() * 1000000000), // Random BIC as BIGINT\n        bicBeneficiario: Math.floor(8000000000 + Math.random() * 1000000000), // Random BIC as BIGINT\n        referencia: Math.random().toString(36).substring(2, 15), // Random reference string\n        endToEndId: Math.random().toString(36).substring(2, 15), // Random EndToEndId\n        instrId: Math.random().toString(36).substring(2, 15), // Random InstrId\n        ibanOrdenante: `GB29NWBK60161331926819${i}`, // Fixed length IBAN\n        ibanBeneficiario: `GB29NWBK60161331926819${i + 100}`, // Fixed length IBAN with offset\n        estados: estados // Embedded estados array\n    };\n\n    // Insert the transaction record into the collection\n    db.transactions.insert(transaction);\n}\n</code></pre>","tags":["MongoDB","PostgreSQL","Work"]},{"location":"PG_vs_Mongo/#query-database_1","title":"Query database","text":"<pre><code>let start = new Date();\ndb.transactions.find({}).limit(0).toArray();  // This doesn\u2019t retrieve any records but allows measuring\nlet end = new Date();\nprint(`Query time: ${end - start} ms`);  // Show query time\n</code></pre>","tags":["MongoDB","PostgreSQL","Work"]},{"location":"Postgres_in_Conda/","title":"Postgres in Conda","text":"","tags":["PostgreSQL","Work","Hobby"]},{"location":"Postgres_in_Conda/#create-conda-environment","title":"Create conda environment","text":"<p><code>conda create --name myenv</code></p>","tags":["PostgreSQL","Work","Hobby"]},{"location":"Postgres_in_Conda/#enter-the-environment","title":"Enter the environment","text":"<p><code>conda activate myenv</code></p>","tags":["PostgreSQL","Work","Hobby"]},{"location":"Postgres_in_Conda/#install-postgresql-via-conda","title":"Install postgresql via conda","text":"<p><code>conda install -y -c conda-forge postgresql</code></p>","tags":["PostgreSQL","Work","Hobby"]},{"location":"Postgres_in_Conda/#create-a-base-database-locally","title":"Create a base database locally","text":"<p><code>initdb -D mylocal_db</code></p>","tags":["PostgreSQL","Work","Hobby"]},{"location":"Postgres_in_Conda/#now-start-the-server-modusinstance-of-postgres","title":"Now start the server modus/instance of postgres","text":"<p><code>pg_ctl -D mylocal_db -l logfile start</code></p> <pre><code>waiting for server to start.... done\nserver started\n</code></pre>","tags":["PostgreSQL","Work","Hobby"]},{"location":"Postgres_in_Conda/#alter-postgres-user-password","title":"Alter postgres user password","text":"<p><code>psql -c \"alter user postgres password 'password'\"</code> </p>","tags":["PostgreSQL","Work","Hobby"]},{"location":"TPA_on_Bare_Metal/","title":"Trusted Postgres Architect on bare metal","text":"","tags":["Enterprisedb","PostgreSQL","Work"]},{"location":"TPA_on_Bare_Metal/#to-use-tpa-on-bare-metal-do-this","title":"To use TPA on bare metal do this:","text":"<ul> <li>Change IP address</li> <li><code>sudo hostnamectl set-hostname &lt;newhostname&gt;</code></li> <li>Using <code>sudo visudo</code> replace <code>%wheel  ALL=(ALL)       ALL</code> with <code>%wheel  ALL=(ALL)       NOPASSWD:ALL</code></li> <li><code>sudo systemctl restart sshd.service</code></li> <li><code>sudo systemctl set-default multi-user.target</code></li> <li><code>sudo dnf groupremove  -y 'GNOME' 'X Window System'</code></li> <li><code>sudo systemctl disable firewalld.service</code></li> <li><code>sudo dnf remove -y firewalld</code></li> <li><code>sudo dnf install chrony &amp;&amp; sudo systemctl enable --now chronyd</code></li> <li>Reboot</li> <li>Copy your SSH key to the targets using <code>ssh-copy-id -i ~/.ssh/id_rsa ton@&lt;target&gt;</code></li> <li>In your config, make sure the correct key and ansible user are defined. <pre><code>keyring_backend: system\nvault_name: b42cba99-6453-4842-907b-4822b61230ea\nssh_key_file: ~/.ssh/id_rsa\n...\ninstance_defaults:\n  platform: bare\n  vars:\n    manage_ssh_hostkeys: no\n    ansible_user: ton\n</code></pre></li> <li>Make sure you create your EDB Repo 2.0 token using <code>export EDB_SUBSCRIPTION_TOKEN=&lt;your token&gt;</code> and check if the variable exists using <code>env</code>.</li> <li>Run <code>tpaexec provision .</code></li> <li>For each host, run <code>ssh-keyscan -H &lt;target&gt; &gt;&gt; tpa_known_hosts</code> to add the targets to the TPA known hosts file.</li> <li>To test use <code>tpaexec ping .</code> <pre><code>red | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nwhite | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nblue | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\npem | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nbarman | SUCCESS =&gt; {\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\n\nreal    0m3.435s\nuser    0m1.773s\nsys     0m0.828s\n</code></pre></li> <li>Create the EDB_SUBSCRIPTION_TOKEN variable using <code>. ./tokens.sh</code>.</li> <li>If all ok, run <code>tpaexec deploy .</code></li> </ul> <p>Important</p> <p>Once you have provisioned the environment on bare metal there is no way back. <code>tpaexec deprovision .</code> removes all TPA assets, but doesn't remove any software from the servers.</p> <p>Get your enterprisedb password <code>tpaexec show-password . enterprisedb</code> (currently <code>Q?LoaBflDcxF%Tp#C0evNvwuu2u29x7o</code>)</p> <p>PEM is running on <code>https://pem:pem</code></p>","tags":["Enterprisedb","PostgreSQL","Work"]},{"location":"greenplum/","title":"Create a Greenplum instance on Rocky 9","text":"<p>This process is run on an Intel i7 with 16Gb memory running Rocky 9.</p>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#prepare-machine","title":"Prepare machine","text":"","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#prepare-os","title":"Prepare OS","text":"","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#disable-selinux","title":"Disable selinux","text":"<ul> <li>In <code>/etc/selinux/config</code> set <code>SELINUX=disabled</code></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#set-limits","title":"Set limits","text":"<ul> <li> <p>In <code>/etc/sysctl.conf</code> set: <pre><code>vm.overcommit_memory = 2\nvm.overcommit_ratio = 95 \nnet.ipv4.ip_local_port_range = 10000 65535 \nkernel.sem = 250 2048000 200 8192\n</code></pre> Enable this with <code>sudo sysctl --system</code></p> </li> <li> <p>In <code>/etc/security/limits.conf</code> set:  <pre><code>soft nofile 524288\nhard nofile 524288\nsoft nproc 131072\nhard nproc 131072\n</code></pre></p> </li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#enable-passwordless-sudo","title":"Enable passwordless sudo","text":"<ul> <li><code>sudo visudo</code> <pre><code>## Allows people in group wheel to run all commands\n#%wheel ALL=(ALL)       ALL\n\n## Same thing without a password\n%wheel ALL=(ALL)       NOPASSWD: ALL\n</code></pre></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#create-user-gpadmin","title":"Create user gpadmin","text":"<ul> <li> <p><code>sudo groupadd gpadmin</code></p> </li> <li> <p><code>sudo useradd gpadmin -r -m -g gpadmin</code></p> </li> <li> <p><code>sudo passwd gpadmin</code> and set the password.</p> </li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#create-ssh-key","title":"Create ssh key","text":"<ul> <li> <p><code>su - gpadmin</code></p> </li> <li> <p><code>ssh-keygen -t rsa -b 4096</code> and copy to host <code>ssh-copy-id -i .ssh/id_rsa gpadmin@&lt;hostname&gt;</code></p> </li> </ul> <p>Reboot to apply all above changes.</p>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#build-greenplum-from-source","title":"Build greenplum from source","text":"","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#install-dependencies","title":"Install dependencies","text":"<ul> <li><code>sudo dnf groupinstall \"Development Tools\"</code></li> <li><code>sudo dnf install python-devel python-pip epel-release perl readline-devel krb5-devel apr-devel libevent-devel libxml2-devel curl-devel bzip2-devel xerces-c-devel</code></li> <li><code>pip install psutil psycopg2 psycopg2-binary</code></li> <li><code>git clone https://github.com/greenplum-db/gpdb-archive</code></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#build-libyaml","title":"Build libyaml","text":"<pre><code>wget https://github.com/yaml/libyaml/archive/refs/tags/0.2.5.tar.gz\ntar -xvzf 0.2.5.tar.gz\ncd libyaml-0.2.5\n./bootstrap\n./configure\nmake\nsudo make install\nexport LDFLAGS=\"-L/usr/local/lib\"\nexport CPPFLAGS=\"-I/usr/local/include\"\n</code></pre>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#build-greenplum","title":"Build greenplum","text":"<ul> <li><code>cd gpdb-archive</code></li> <li><code>./configure --with-perl --with-python --with-libxml --with-gssapi --prefix=/usr/local/gpdb</code></li> <li><code>make -j8</code></li> <li><code>sudo make install</code></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#greenplum-post-installation","title":"Greenplum post installation","text":"","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#add-greenplum-path-to-bashrc-of-the-gpadmin-user","title":"Add greenplum path to <code>.bashrc</code> of the <code>gpadmin</code> user.","text":"<ul> <li>As user gpadmin, <code>echo \". /usr/local/gpdb/greenplum_path.sh\" &gt;&gt; .bashrc</code></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#create-a-host-file-with-all-your-greenplum-nodes-in-it-for-me-this-is-just-one-node","title":"Create a host file with all your greenplum nodes in it (for me this is just one node)","text":"<ul> <li>As user gpadmin, <code>echo \"&lt;hostname&gt; &gt; hostfile</code></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#create-data-directory","title":"Create data directory","text":"<ul> <li> <p>As user gpadmin, <code>mkdir ./data</code></p> </li> <li> <p><code>vim .bashrc</code> <pre><code>export MASTER_DATA_DIRECTORY=/home/gpadmin/data/gpsne-1\n</code></pre></p> </li> <li><code>source .bashrc</code></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#configure-greenplum-cluster","title":"Configure greenplum cluster","text":"","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#copy-sample-configuration-to-home","title":"Copy sample configuration to $HOME","text":"<ul> <li>As gpadmin, <code>cp $GPHOME/docs/cli_help/gpconfigs/gpinitsystem_singlenode ~</code></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#modify-config-file","title":"Modify config file","text":"<ul> <li><code>vim gpinitsystem_singlenode</code> and change: <pre><code>MACHINE_LIST_FILE=./hostfile\ndeclare -a DATA_DIRECTORY=(/home/gpadmin/data /home/gpadmin/data /home/gpadmin/data) # I created 3 shards, create as many as you need.\nCOORDINATOR_HOSTNAME=&lt;hostname&gt;\nCOORDINATOR_DIRECTORY=/home/gpadmin/data\n</code></pre></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#create-cluster","title":"Create cluster","text":"<ul> <li>As gpadmin, <code>gpinitsystem -c gpinitsystem_singlenode</code></li> </ul> <p>Greenplum should now be running <pre><code>gpadmin    10780       1  0 12:19 ?        00:00:00 /usr/local/gpdb/bin/postgres -D /home/gpadmin/data/gpsne0 -c gp_role=execute\ngpadmin    10781       1  0 12:19 ?        00:00:00 /usr/local/gpdb/bin/postgres -D /home/gpadmin/data/gpsne1 -c gp_role=execute\ngpadmin    10782       1  0 12:19 ?        00:00:00 /usr/local/gpdb/bin/postgres -D /home/gpadmin/data/gpsne2 -c gp_role=execute\ngpadmin    10783   10780  0 12:19 ?        00:00:00 postgres:  6000, logger process   \ngpadmin    10784   10781  0 12:19 ?        00:00:00 postgres:  6001, logger process   \ngpadmin    10785   10782  0 12:19 ?        00:00:00 postgres:  6002, logger process   \ngpadmin    10789   10780  0 12:19 ?        00:00:00 postgres:  6000, checkpointer   \ngpadmin    10790   10780  0 12:19 ?        00:00:00 postgres:  6000, background writer   \ngpadmin    10791   10780  0 12:19 ?        00:00:00 postgres:  6000, walwriter   \ngpadmin    10792   10780  0 12:19 ?        00:00:00 postgres:  6000, autovacuum launcher   \ngpadmin    10793   10780  0 12:19 ?        00:00:00 postgres:  6000, stats collector   \ngpadmin    10794   10780  0 12:19 ?        00:00:00 postgres:  6000, logical replication launcher   \ngpadmin    10795   10781  0 12:19 ?        00:00:00 postgres:  6001, checkpointer   \ngpadmin    10796   10781  0 12:19 ?        00:00:00 postgres:  6001, background writer   \ngpadmin    10797   10781  0 12:19 ?        00:00:00 postgres:  6001, walwriter   \ngpadmin    10798   10781  0 12:19 ?        00:00:00 postgres:  6001, autovacuum launcher   \ngpadmin    10799   10781  0 12:19 ?        00:00:00 postgres:  6001, stats collector   \ngpadmin    10800   10781  0 12:19 ?        00:00:00 postgres:  6001, logical replication launcher   \ngpadmin    10801   10782  0 12:19 ?        00:00:00 postgres:  6002, checkpointer   \ngpadmin    10802   10782  0 12:19 ?        00:00:00 postgres:  6002, background writer   \ngpadmin    10803   10782  0 12:19 ?        00:00:00 postgres:  6002, walwriter   \ngpadmin    10804   10782  0 12:19 ?        00:00:00 postgres:  6002, autovacuum launcher   \ngpadmin    10805   10782  0 12:19 ?        00:00:00 postgres:  6002, stats collector   \ngpadmin    10806   10782  0 12:19 ?        00:00:00 postgres:  6002, logical replication launcher   \ngpadmin    10809       1  0 12:19 ?        00:00:00 /usr/local/gpdb/bin/postgres -D /home/gpadmin/data/gpsne-1 -c gp_role=dispatch\ngpadmin    10810   10809  0 12:19 ?        00:00:00 postgres:  5432, master logger process   \ngpadmin    10812   10809  0 12:19 ?        00:00:00 postgres:  5432, checkpointer   \ngpadmin    10813   10809  0 12:19 ?        00:00:00 postgres:  5432, background writer   \ngpadmin    10814   10809  0 12:19 ?        00:00:00 postgres:  5432, walwriter   \ngpadmin    10815   10809  0 12:19 ?        00:00:00 postgres:  5432, autovacuum launcher   \ngpadmin    10816   10809  0 12:19 ?        00:00:00 postgres:  5432, stats collector   \ngpadmin    10817   10809  0 12:19 ?        00:00:00 postgres:  5432, dtx recovery process   \ngpadmin    10818   10809  0 12:19 ?        00:00:00 postgres:  5432, ftsprobe process   \ngpadmin    10833   10809  0 12:19 ?        00:00:00 postgres:  5432, logical replication launcher\n</code></pre></p>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#after-cluster-creation-add-data-directory-to-environment","title":"After cluster creation add data directory to environment.","text":"<ul> <li>As gpadmin, <code>echo \"COORDINATOR_DATA_DIRECTORY=/home/gpadmin/data/gpsne-1\" &gt;&gt; .bashrc</code></li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#test-cluster","title":"Test cluster","text":"<ul> <li> <p>As gpadmin, <code>createdb test</code></p> </li> <li> <p><code>psql test</code> <pre><code>psql (12.12)\nType \"help\" for help.\n\ntest=# \\timing\nTiming is on.\n\ntest=# create table t1 as select generate_series(1,1000000) as colA distributed by (colA);\nSELECT 1000000\nTime: 683.351 ms\n\ntest=# select count(*) from t1;\n  count  \n---------\n 1000000\n(1 row)\nTime: 164.377 ms\n\ntest=# select sum(colA) from t1;\n     sum      \n--------------\n 500000500000\n(1 row)\nTime: 89.030 ms\n</code></pre></p> </li> </ul>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#epas-17","title":"EPAS 17","text":"<pre><code>[enterprisedb@onion ~]$ psql edb\npsql (17.2.0)\nType \"help\" for help.\n\nedb=# \\timing\nTiming is on.\n\nedb=# create table t1 as select generate_series(1,1000000);\nSELECT 1000000\nTime: 1483.358 ms (00:01.483)\n\nedb=# select count(*) from t1;\n  count  \n---------\n 1000000\n(1 row)\nTime: 67.455 ms\n\nedb=# select sum(generate_series) from t1;\n     sum      \n--------------\n 500000500000\n(1 row)\nTime: 61.911 ms\n</code></pre>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#remove-the-system","title":"Remove the system","text":"<p>Normally you should be able to remove the system using <code>gpdeletesystem</code>. If you get the error <code>gpadmin-[CRITICAL]:-Error deleting system: FATAL:  System was started in single node mode - only utility mode connections are allowed</code>, then just stop the cluster using <code>gpstop</code> and remove the directoes and files under the <code>./data</code> directory. There must be a better way, but i haven't figured that out yet.</p>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#query-csv-files-from-storage","title":"Query CSV files from storage","text":"<p>The intent is to be able to query the many CSV files i have on a CIFS volume using a foreign data wrapper.</p>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#mount-the-smb-share-on-the-server","title":"Mount the SMB share on the server","text":"","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#install-samba-if-not-already-installed","title":"Install Samba if not already installed","text":"<pre><code>[ton@onion ~]$ sudo dnf install cifs-utils samba samba-common samba-client\nLast metadata expiration check: 2:52:41 ago on Thu 02 Jan 2025 10:06:42 AM CET.\nPackage cifs-utils-7.0-5.el9.x86_64 is already installed.\nPackage samba-common-4.20.2-2.el9_5.noarch is already installed.\nPackage samba-client-4.20.2-2.el9_5.x86_64 is already installed.\nDependencies resolved.\n=============================================================================================================\n Package                            Architecture       Version                      Repository          Size\n=============================================================================================================\nInstalling:\n samba                              x86_64             4.20.2-2.el9_5               baseos             939 k\nInstalling dependencies:\n libnetapi                          x86_64             4.20.2-2.el9_5               baseos             143 k\n samba-common-tools                 x86_64             4.20.2-2.el9_5               baseos             483 k\n samba-dcerpc                       x86_64             4.20.2-2.el9_5               baseos             716 k\n samba-ldb-ldap-modules             x86_64             4.20.2-2.el9_5               baseos              27 k\n samba-libs                         x86_64             4.20.2-2.el9_5               baseos             124 k\n\nTransaction Summary\n=============================================================================================================\nInstall  6 Packages\n\nTotal download size: 2.4 M\nInstalled size: 8.6 M\nIs this ok [y/N]: y\nDownloading Packages:\n(1/6): samba-libs-4.20.2-2.el9_5.x86_64.rpm                                  353 kB/s | 124 kB     00:00    \n(2/6): samba-ldb-ldap-modules-4.20.2-2.el9_5.x86_64.rpm                      414 kB/s |  27 kB     00:00    \n(3/6): samba-common-tools-4.20.2-2.el9_5.x86_64.rpm                          979 kB/s | 483 kB     00:00    \n(4/6): samba-4.20.2-2.el9_5.x86_64.rpm                                       1.7 MB/s | 939 kB     00:00    \n(5/6): samba-dcerpc-4.20.2-2.el9_5.x86_64.rpm                                2.5 MB/s | 716 kB     00:00    \n(6/6): libnetapi-4.20.2-2.el9_5.x86_64.rpm                                   358 kB/s | 143 kB     00:00    \n-------------------------------------------------------------------------------------------------------------\nTotal                                                                        2.1 MB/s | 2.4 MB     00:01     \nRunning transaction check\nTransaction check succeeded.\nRunning transaction test\nTransaction test succeeded.\nRunning transaction\n  Preparing        :                                                                                     1/1 \n  Installing       : libnetapi-4.20.2-2.el9_5.x86_64                                                     1/6 \n  Installing       : samba-libs-4.20.2-2.el9_5.x86_64                                                    2/6 \n  Installing       : samba-dcerpc-4.20.2-2.el9_5.x86_64                                                  3/6 \n  Installing       : samba-ldb-ldap-modules-4.20.2-2.el9_5.x86_64                                        4/6 \n  Installing       : samba-common-tools-4.20.2-2.el9_5.x86_64                                            5/6 \n  Installing       : samba-4.20.2-2.el9_5.x86_64                                                         6/6 \n  Running scriptlet: samba-4.20.2-2.el9_5.x86_64                                                         6/6 \n  Verifying        : samba-libs-4.20.2-2.el9_5.x86_64                                                    1/6 \n  Verifying        : samba-common-tools-4.20.2-2.el9_5.x86_64                                            2/6 \n  Verifying        : samba-4.20.2-2.el9_5.x86_64                                                         3/6 \n  Verifying        : samba-ldb-ldap-modules-4.20.2-2.el9_5.x86_64                                        4/6 \n  Verifying        : samba-dcerpc-4.20.2-2.el9_5.x86_64                                                  5/6 \n  Verifying        : libnetapi-4.20.2-2.el9_5.x86_64                                                     6/6 \n\nInstalled:\n  libnetapi-4.20.2-2.el9_5.x86_64                            samba-4.20.2-2.el9_5.x86_64                     \n  samba-common-tools-4.20.2-2.el9_5.x86_64                   samba-dcerpc-4.20.2-2.el9_5.x86_64              \n  samba-ldb-ldap-modules-4.20.2-2.el9_5.x86_64               samba-libs-4.20.2-2.el9_5.x86_64                \n\nComplete!\n</code></pre>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#create-directory-where-to-mount-the-smb-share","title":"Create directory where to mount the SMB share.","text":"<pre><code>[ton@onion ~]$ mkdir /mnt/usbdrive\n</code></pre>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#create-a-samba-credentials-file","title":"Create a Samba credentials file.","text":"<pre><code>[ton@onion ~]$ sudo cat - &gt; /root/.smbcredentials_USB\nusername=ton\npassword=&lt;password&gt;\n^Z\n[ton@onion ~]$ chmod 600 /root/.smbcredentials_USB\n</code></pre>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#test-mount-the-share-the-add-it-to-etcfstab","title":"Test mount the share, the add it to <code>/etc/fstab</code>.","text":"<pre><code>[ton@onion ~]$ sudo mount -t cifs //192.168.0.10/USB /mnt/usbdrive -o credentials=/root/.smbcredentials_USB\n[ton@onion ~]$ cd /mnt/usbdrive/\n[ton@onion usbdrive]$ ls\n Breaches        iPhone          Ringtones         Videos\n'Curious Moon'   ISOs            ROMS              Webs\n Downloads      'Motos Coches'   Shared            Work\n Drivers         Personal        Software         'xBox 360'\n eBooks          Photos         'Surabhi Pandey'\n Hacking         Radio           UFO\n\n[ton@onion ~]$ umount /mnt/usbdrive\n[ton@onion ~]$ echo \"//192.168.0.10/USB /mnt/usbdrive cifs ro,auto,credentials=/root/.smbcredentials_USB 0 0\" &gt;&gt; /etc/fstab\n[ton@onion ~]$ sudo systemctl daemon-reload\n[ton@onion ~]$ sudo mount -a -v\n/                        : ignored\n/boot                    : already mounted\n/boot/efi                : already mounted\n/home                    : already mounted\nnone                     : ignored\nHost \"192.168.0.10\" resolved to the following IP addresses: 192.168.0.10\nmount.cifs kernel mount options: ip=192.168.0.10,unc=\\\\192.168.0.10\\USB,user=ton,pass=********\n/mnt/usbdrive            : successfully mounted\n[ton@onion ~]$ ls /mnt/usbdrive\n Breaches        Downloads   eBooks    iPhone  'Motos Coches'   Photos   Ringtones   Shared    'Surabhi Pandey'   Videos   Work\n'Curious Moon'   Drivers     Hacking   ISOs     Personal        Radio    ROMS        Software   UFO               Webs    'xBox 360'\n</code></pre>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#not-that-the-server-is-prepared-lets-configure-the-database","title":"Not that the server is prepared, let's configure the database.","text":"","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#create-the-file_fdw-extension","title":"Create the <code>file_fdw</code> extension.","text":"<pre><code>\u2717 ~ psql -h onion -p 5432 -U gpadmin breaches\npsql (17.2 (Homebrew), server 12.12)\nType \"help\" for help.\n\nbreaches=# \\dx\n                                List of installed extensions\n      Name       | Version |   Schema   |                    Description                    \n-----------------+---------+------------+---------------------------------------------------\n gp_exttable_fdw | 1.0     | pg_catalog | External Table Foreign Data Wrapper for Greenplum\n gp_toolkit      | 1.6     | gp_toolkit | various GPDB administrative views/functions\n plpgsql         | 1.0     | pg_catalog | PL/pgSQL procedural language\n(3 rows)\n\nbreaches=# create extension file_fdw;\nCREATE EXTENSION\n\nbreaches=# \\dx\n                                List of installed extensions\n      Name       | Version |   Schema   |                    Description                    \n-----------------+---------+------------+---------------------------------------------------\n file_fdw        | 1.0     | public     | foreign-data wrapper for flat file access\n gp_exttable_fdw | 1.0     | pg_catalog | External Table Foreign Data Wrapper for Greenplum\n gp_toolkit      | 1.6     | gp_toolkit | various GPDB administrative views/functions\n plpgsql         | 1.0     | pg_catalog | PL/pgSQL procedural language\n(4 rows)\n</code></pre>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#create-the-server-and-the-table-definition","title":"Create the server and the table definition.","text":"<pre><code>breaches=# CREATE SERVER csv_server FOREIGN DATA WRAPPER file_fdw;\nCREATE SERVER\nbreaches=# CREATE FOREIGN TABLE accounts (email TEXT, password TEXT) SERVER csv_server OPTIONS (filename '/mnt/usbdrive/Breaches/120k.txt',delimiter ':',null 'NULL', header 'false');\nCREATE FOREIGN TABLE\nbreaches=# select * from accounts;\n                       email                        |                              password                              \n----------------------------------------------------+--------------------------------------------------------------------\n 00_gerard00_@yahoo.com.ph                          | 001gerard\n 01010101@meralco.com.ph                            | nghao\n 012089_neil@yahoo.com.ph                           | music6\n 015_jericho@yahoo.com.ph                           | jericho\n</code></pre>","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"greenplum/#next-is-to-find-a-way-to-query-all-files-in-the-directory-structure","title":"Next is to find a way to query ALL files in the directory structure.","text":"","tags":["Greenplum","Rocky 9","PostgreSQL","Work"]},{"location":"pgAI/","title":"pgAI","text":"","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#installation-in-postgresql-17-on-debian-12-lxc","title":"Installation in PostgreSQL 17 on Debian 12 (LXC)","text":"<p>The intent is to be able to query the many CSV files i have on a local harddrive.</p>","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#install-edb-repos","title":"Install EDB repos.","text":"<pre><code>root@postgresql:~# apt install edb-pg17-aidb edb-pg17-pgfs edb-pg17-pgvector-0\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following NEW packages will be installed:\n  edb-pg17-aidb edb-pg17-pgfs edb-pg17-pgvector-0\n0 upgraded, 3 newly installed, 0 to remove and 0 not upgraded.\nNeed to get 6,634 kB of archives.\nAfter this operation, 36.8 MB of additional disk space will be used.\nGet:1 https://downloads.enterprisedb.com/pdZe6pcnWIgmuqdR7v1L38rG6Z6wJEsY/enterprise/deb/debian bookworm/main amd64 edb-pg17-aidb amd64 1.0.7-1.bookworm [6,179 kB]\nGet:2 https://downloads.enterprisedb.com/pdZe6pcnWIgmuqdR7v1L38rG6Z6wJEsY/enterprise/deb/debian bookworm/main amd64 edb-pg17-pgfs amd64 1.0.4-1.bookworm [157 kB]\nGet:3 https://downloads.enterprisedb.com/pdZe6pcnWIgmuqdR7v1L38rG6Z6wJEsY/enterprise/deb/debian bookworm/main amd64 edb-pg17-pgvector-0 amd64 0.8.0-3.bookworm [299 kB]\nFetched 6,634 kB in 2s (3,670 kB/s)      \nSelecting previously unselected package edb-pg17-aidb.\n(Reading database ... 22920 files and directories currently installed.)\nPreparing to unpack .../edb-pg17-aidb_1.0.7-1.bookworm_amd64.deb ...\nUnpacking edb-pg17-aidb (1.0.7-1.bookworm) ...\nSelecting previously unselected package edb-pg17-pgfs.\nPreparing to unpack .../edb-pg17-pgfs_1.0.4-1.bookworm_amd64.deb ...\nUnpacking edb-pg17-pgfs (1.0.4-1.bookworm) ...\nSelecting previously unselected package edb-pg17-pgvector-0.\nPreparing to unpack .../edb-pg17-pgvector-0_0.8.0-3.bookworm_amd64.deb ...\nUnpacking edb-pg17-pgvector-0 (0.8.0-3.bookworm) ...\nSetting up edb-pg17-aidb (1.0.7-1.bookworm) ...\nSetting up edb-pg17-pgvector-0 (0.8.0-3.bookworm) ...\nSetting up edb-pg17-pgfs (1.0.4-1.bookworm) ...\nProcessing triggers for postgresql-common (267.pgdg120+1) ...\nBuilding PostgreSQL dictionaries from installed myspell/hunspell packages...\nRemoving obsolete dictionary files:\nroot@postgresql:~#\n</code></pre>","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#connect-to-db-as-user-postgres","title":"Connect to DB as user <code>postgres</code>","text":"<pre><code>root@postgresql:~# psql -h localhost -U postgres\nPassword for user postgres: \npsql (17.2 (Debian 17.2-1.pgdg120+1))\nSSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, compression: off, ALPN: postgresql)\nType \"help\" for help.\n</code></pre>","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#create-extensions","title":"Create extensions","text":"<pre><code>postgres=# CREATE EXTENSION aidb CASCADE;\nNOTICE:  installing required extension \"vector\"\nCREATE EXTENSION\npostgres=# CREATE EXTENSION pgfs;\nCREATE EXTENSION\n</code></pre>","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#check-extensions","title":"Check extensions","text":"<pre><code>postgres=# \\dx\n                                List of installed extensions\n  Name   | Version |   Schema   |                        Description                         \n---------+---------+------------+------------------------------------------------------------\n aidb    | 1.0.7   | aidb       | aidb: makes it easy to build AI applications with postgres\n pgfs    | 1.0.4   | pgfs       | pgfs: enables access to filesystem-like storage locations\n plpgsql | 1.0     | pg_catalog | PL/pgSQL procedural language\n vector  | 0.8.0   | public     | vector data type and ivfflat and hnsw access methods\n(4 rows)\n\npostgres=# \n</code></pre>","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#which-models-can-i-use","title":"Which models can i use?","text":"<pre><code>postgres=# SELECT * FROM aidb.list_registered_models();\n name  |  provider  |    options    \n-------+------------+---------------\n bert  | bert_local | {\"config={}\"}\n clip  | clip_local | {\"config={}\"}\n t5    | t5_local   | {\"config={}\"}\n dummy | dummy      | {\"config={}\"}\n(4 rows)\n\npostgres=#\n</code></pre> <p>BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model that is used for natural language processing tasks such as text classification, text generation, and text completion.</p> <p>CLIP (Contrastive Language-Image Pre-training) is a model that learns visual concepts from natural language supervision.</p> <p>T5 is a text-to-text transformer model that converts input text into output text.</p> <p>DUMMY is a placeholder model that can be used for testing purposes.</p> <p>Other models we can use are <code>openai_embeddings</code> and <code>openai_completions</code>.</p>","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#creating-a-retriever-for-locally-stored-csv-files","title":"Creating a retriever for locally stored CSV files.","text":"","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#create-a-storage-location","title":"Create a storage location","text":"","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#_1","title":"pgAI","text":"","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"pgAI/#create-a-new-volume","title":"Create a new volume","text":"","tags":["Enterprisedb","Postgres AI","Hybrid Control Plane","Cloud","PostgreSQL","Work"]},{"location":"tags/","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"tags/#ai","title":"AI","text":"<ul> <li>How to Use Hugging Face Models with Ollama</li> </ul>"},{"location":"tags/#aws","title":"AWS","text":"<ul> <li>Kafka on AWS</li> </ul>"},{"location":"tags/#cloud","title":"Cloud","text":"<ul> <li>Installation EDB Postgres AI Control Plane on kind</li> <li>pgAI</li> </ul>"},{"location":"tags/#enterprisedb","title":"Enterprisedb","text":"<ul> <li>Installation EDB Postgres AI Control Plane on kind</li> <li>Trusted Postgres Architect on bare metal</li> <li>pgAI</li> </ul>"},{"location":"tags/#greenplum","title":"Greenplum","text":"<ul> <li>Create a Greenplum instance on Rocky 9</li> </ul>"},{"location":"tags/#hobby","title":"Hobby","text":"<ul> <li>Kafka on AWS</li> <li>How to Use Hugging Face Models with Ollama</li> <li>Postgres in Conda</li> </ul>"},{"location":"tags/#hybrid-control-plane","title":"Hybrid Control Plane","text":"<ul> <li>Installation EDB Postgres AI Control Plane on kind</li> <li>pgAI</li> </ul>"},{"location":"tags/#kafka","title":"Kafka","text":"<ul> <li>Kafka on AWS</li> </ul>"},{"location":"tags/#mongodb","title":"MongoDB","text":"<ul> <li>PostgreSQL vs. MongoDB</li> </ul>"},{"location":"tags/#postgresql","title":"PostgreSQL","text":"<ul> <li>Installation EDB Postgres AI Control Plane on kind</li> <li>PostgreSQL vs. MongoDB</li> <li>Postgres in Conda</li> <li>Trusted Postgres Architect on bare metal</li> <li>Create a Greenplum instance on Rocky 9</li> <li>pgAI</li> </ul>"},{"location":"tags/#postgres-ai","title":"Postgres AI","text":"<ul> <li>Installation EDB Postgres AI Control Plane on kind</li> <li>pgAI</li> </ul>"},{"location":"tags/#rocky-9","title":"Rocky 9","text":"<ul> <li>Create a Greenplum instance on Rocky 9</li> </ul>"},{"location":"tags/#work","title":"Work","text":"<ul> <li>Installation EDB Postgres AI Control Plane on kind</li> <li>How to Use Hugging Face Models with Ollama</li> <li>PostgreSQL vs. MongoDB</li> <li>Postgres in Conda</li> <li>Trusted Postgres Architect on bare metal</li> <li>Create a Greenplum instance on Rocky 9</li> <li>pgAI</li> </ul>"}]}